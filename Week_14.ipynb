{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLVZ+cE6MVhTtF3NynW5AF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ASN-Lab/Big-Data/blob/main/Week_14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week 14 (Spark Mlib for Machine Learning)"
      ],
      "metadata": {
        "id": "PFUYL30BvQ-T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Practice"
      ],
      "metadata": {
        "id": "cGzwzChHvVE9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Spark Mlib"
      ],
      "metadata": {
        "id": "BX75rZ77vg1v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-E3apamEvGo9",
        "outputId": "db16187f-8d36-4947-a8be-d8f904fef453"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients: [0.9999999999999992]\n",
            "Intercept: 15.000000000000009\n"
          ]
        }
      ],
      "source": [
        "# Example: Linear Regression with Spark MLlib\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName('MLlib Example').getOrCreate()\n",
        "\n",
        "# Load sample data\n",
        "data = [(1, 5.0, 20.0), (2, 10.0, 25.0), (3, 15.0, 30.0), (4, 20.0, 35.0)]\n",
        "columns = ['ID', 'Feature', 'Target']\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Prepare data for modeling\n",
        "assembler = VectorAssembler(inputCols=['Feature'], outputCol='Features')\n",
        "df_transformed = assembler.transform(df)\n",
        "\n",
        "# Train a linear regression model\n",
        "lr = LinearRegression(featuresCol='Features', labelCol='Target')\n",
        "model = lr.fit(df_transformed)\n",
        "\n",
        "# Print model coefficients\n",
        "print(f'Coefficients: {model.coefficients}')\n",
        "print(f'Intercept: {model.intercept}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Logistic Regression"
      ],
      "metadata": {
        "id": "Pc_A-_WivtH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Practice: Logistic Regression\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Example dataset\n",
        "# Original: data = [(1, [2.0, 3.0], 0), ...] where 'RawFeatures' was array<double>\n",
        "# Problem: VectorAssembler does not accept array<double> as an inputCol directly.\n",
        "# Solution: Treat the elements of the array as separate features by restructuring data.\n",
        "data = [(1, 2.0, 3.0, 0), (2, 1.0, 5.0, 1), (3, 2.5, 4.5, 1), (4, 3.0, 6.0, 0)]\n",
        "columns = ['ID', 'Feature1', 'Feature2', 'Label'] # Define separate columns for features\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Prepare data for modeling: assemble 'Feature1' and 'Feature2' into a vector column named 'Features'\n",
        "assembler = VectorAssembler(inputCols=['Feature1', 'Feature2'], outputCol='Features')\n",
        "df_transformed = assembler.transform(df)\n",
        "\n",
        "# Train logistic regression model\n",
        "lr = LogisticRegression(featuresCol='Features', labelCol='Label')\n",
        "model = lr.fit(df_transformed) # Use the transformed DataFrame\n",
        "\n",
        "# Display coefficients and summary\n",
        "print(f'Coefficients: {model.coefficients}')\n",
        "print(f'Intercept: {model.intercept}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHafFeqGvwhv",
        "outputId": "62cccff1-166c-4f25-cc3f-a8ed024c6ba7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients: [-12.262057929180484,4.087352266486688]\n",
            "Intercept: 11.56891272665312\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###kMeans Clustering"
      ],
      "metadata": {
        "id": "lhkVSt2CwSp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Practice: KMeans Clustering\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.linalg import Vectors # Import Vectors for DenseVector\n",
        "\n",
        "# Example dataset\n",
        "# Use Vectors.dense to create a DenseVector for the features column\n",
        "data = [(1, Vectors.dense([1.0, 1.0])),\n",
        "        (2, Vectors.dense([5.0, 5.0])),\n",
        "        (3, Vectors.dense([10.0, 10.0])),\n",
        "        (4, Vectors.dense([15.0, 15.0]))]\n",
        "columns = ['ID', 'Features']\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Train KMeans clustering model\n",
        "kmeans = KMeans(featuresCol='Features', k=2)\n",
        "model = kmeans.fit(df)\n",
        "\n",
        "# Show cluster centers\n",
        "centers = model.clusterCenters()\n",
        "print(f'Cluster Centers: {centers}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZOZkrHPwV3t",
        "outputId": "fb3c25b7-6fcd-4bd2-f233-f27c6fd77a86"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster Centers: [array([12.5, 12.5]), array([3., 3.])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Homework"
      ],
      "metadata": {
        "id": "H0fF23jtwnEC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Inisialisasi & load dataset"
      ],
      "metadata": {
        "id": "rO-VAehU5XX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ML_Comparison\") \\\n",
        "    .config(\"spark.driver.memory\", \"6g\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark\n",
        "\n",
        "# Load dataset\n",
        "csv_path = \"/content/full.csv\"\n",
        "\n",
        "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(csv_path)\n",
        "df.printSchema()\n",
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hawNsFedwpO3",
        "outputId": "9416512e-c857-4ab2-8a76-6ca21d845862"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- commit: string (nullable = true)\n",
            " |-- author: string (nullable = true)\n",
            " |-- date: string (nullable = true)\n",
            " |-- message: string (nullable = true)\n",
            " |-- repo: string (nullable = true)\n",
            "\n",
            "+--------------------+--------------------+--------------------+--------------------+-----------------+\n",
            "|              commit|              author|                date|             message|             repo|\n",
            "+--------------------+--------------------+--------------------+--------------------+-----------------+\n",
            "|692bba578efb5e305...|Mortada Mehyar <m...|Wed Apr 21 12:27:...|DOC: add example ...|pandas-dev/pandas|\n",
            "|855696cde0ef5d80a...|Patrick Hoefler <...|Wed Apr 21 01:23:...|Add keyword sort ...|pandas-dev/pandas|\n",
            "|eaaefd140289a5103...|attack68 <2425655...|Wed Apr 21 01:21:...|ENH: `Styler.high...|pandas-dev/pandas|\n",
            "|aab87997058f3c74b...|attack68 <2425655...|Wed Apr 21 01:01:...|ENH: add `decimal...|pandas-dev/pandas|\n",
            "|9c43cd7675d961740...|Simon Hawkins <si...|Tue Apr 20 23:58:...|[ArrowStringArray...|pandas-dev/pandas|\n",
            "+--------------------+--------------------+--------------------+--------------------+-----------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data cleaning"
      ],
      "metadata": {
        "id": "LbNfFKdq5cbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Deduplikasi\n",
        "from pyspark.sql.functions import col, when, to_timestamp, hour, dayofweek, length\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "df = df.dropDuplicates()\n",
        "df = df.na.drop(subset=[\"message\"])\n",
        "\n",
        "# Konversi timestamp\n",
        "df = df.withColumn(\"date\", to_timestamp(\"date\"))\n",
        "df = df.filter(col(\"date\").isNotNull())\n",
        "\n",
        "# Feature engineering\n",
        "df = (df\n",
        "      .withColumn(\"hour\", hour(\"date\"))\n",
        "      .withColumn(\"day_of_week\", dayofweek(\"date\"))\n",
        "      .withColumn(\"msg_len\", length(\"message\"))\n",
        "     )\n",
        "\n",
        "# Labeling for classification: Create a binary label based on msg_len\n",
        "# Calculate the median message length\n",
        "median_msg_len = df.approxQuantile(\"msg_len\", [0.5], 0.01)[0]\n",
        "df = df.withColumn(\"label_class\", when(col(\"msg_len\") > median_msg_len, 1).otherwise(0).cast(IntegerType()))\n",
        "\n",
        "print(f\"Created 'label_class' column based on median 'msg_len' ({median_msg_len:.2f}).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huwbJpxb5fiP",
        "outputId": "6f32697c-4020-4369-f774-159ae716f405"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 'label_class' column based on median 'msg_len' (6.00).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Feature & pipelin"
      ],
      "metadata": {
        "id": "6ZIEQE-z6oY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler, MinMaxScaler\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "feature_cols = [\"hour\", \"day_of_week\", \"msg_len\"]\n",
        "\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaled_features\")"
      ],
      "metadata": {
        "id": "Uiy85CiZ6v1p"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training"
      ],
      "metadata": {
        "id": "9VNr4dvz63i7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = df.randomSplit([0.8, 0.2], seed=42)"
      ],
      "metadata": {
        "id": "nTxSPiEs68G3"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Clasification: Logistic regression model"
      ],
      "metadata": {
        "id": "jwQsH5jv6_4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "lr_clf = LogisticRegression(\n",
        "    featuresCol=\"scaled_features\",\n",
        "    labelCol=\"label_class\",\n",
        "    maxIter=20\n",
        ")\n",
        "\n",
        "pipeline_clf = Pipeline(stages=[assembler, scaler, lr_clf])\n",
        "model_clf = pipeline_clf.fit(train)\n",
        "\n",
        "pred_clf = model_clf.transform(test)\n",
        "pred_clf.select(\"scaled_features\", \"label_class\", \"prediction\").show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxNOlzAx7Cst",
        "outputId": "9d4f6fb5-db68-4692-b7f4-1432ee51b726"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----------+----------+\n",
            "|     scaled_features|label_class|prediction|\n",
            "+--------------------+-----------+----------+\n",
            "|[0.0,0.6666666666...|          0|       0.0|\n",
            "|[0.0,0.3333333333...|          1|       1.0|\n",
            "|[0.0,0.6666666666...|          1|       1.0|\n",
            "|[0.0,0.8333333333...|          1|       1.0|\n",
            "|[0.0,0.8333333333...|          1|       1.0|\n",
            "|[0.0,0.1666666666...|          0|       0.0|\n",
            "|[0.0,0.1666666666...|          1|       1.0|\n",
            "|[0.0,0.1666666666...|          0|       0.0|\n",
            "|[0.0,0.6666666666...|          1|       1.0|\n",
            "|           (3,[],[])|          0|       0.0|\n",
            "+--------------------+-----------+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "evaluator_clf = BinaryClassificationEvaluator(\n",
        "    labelCol=\"label_class\",\n",
        "    rawPredictionCol=\"rawPrediction\",\n",
        "    metricName=\"areaUnderROC\"\n",
        ")\n",
        "\n",
        "auc = evaluator_clf.evaluate(pred_clf)\n",
        "print(\"AUC Logistic Regression =\", auc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dupOJ_Jc9A2y",
        "outputId": "f48a54f1-d60f-4aa5-83f2-88de7ba18370"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC Logistic Regression = 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Regresi: Linear regression model"
      ],
      "metadata": {
        "id": "b_QmOZ-t9bqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "lr_reg = LinearRegression(\n",
        "    featuresCol=\"scaled_features\",\n",
        "    labelCol=\"msg_len\",\n",
        "    maxIter=20\n",
        ")\n",
        "\n",
        "pipeline_reg = Pipeline(stages=[assembler, scaler, lr_reg])\n",
        "model_reg = pipeline_reg.fit(train)\n",
        "\n",
        "pred_reg = model_reg.transform(test)\n",
        "pred_reg.select(\"msg_len\", \"prediction\").show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DalToZe9lKQ",
        "outputId": "2cdb8916-ca74-4970-96c3-1c1628a79c8f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+\n",
            "|msg_len|        prediction|\n",
            "+-------+------------------+\n",
            "|      5| 4.999999999999998|\n",
            "|     11| 11.00000000000002|\n",
            "|     18|18.000000000000007|\n",
            "|     18|17.999999999999996|\n",
            "|     18|17.999999999999996|\n",
            "|      5| 5.000000000000025|\n",
            "|     29|29.000000000000036|\n",
            "|      5| 5.000000000000025|\n",
            "|     11|11.000000000000002|\n",
            "|      5| 5.000000000000034|\n",
            "+-------+------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "evaluator_reg = RegressionEvaluator(\n",
        "    labelCol=\"msg_len\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"rmse\"\n",
        ")\n",
        "\n",
        "rmse = evaluator_reg.evaluate(pred_reg)\n",
        "print(\"RMSE Linear Regression =\", rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmVEQvlDB-u3",
        "outputId": "dfc97bce-1c4c-415c-d7c1-0826b285b75d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE Linear Regression = 1.9928651389931068e-14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Hyperparameter"
      ],
      "metadata": {
        "id": "6MmpyGvJClix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic regression\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "\n",
        "param_grid_clf = (ParamGridBuilder()\n",
        "                  .addGrid(lr_clf.regParam, [0.01, 0.1, 1.0])\n",
        "                  .addGrid(lr_clf.elasticNetParam, [0.0, 0.5, 1.0])\n",
        "                  .build())\n",
        "\n",
        "cv_clf = CrossValidator(\n",
        "    estimator=pipeline_clf,\n",
        "    estimatorParamMaps=param_grid_clf,\n",
        "    evaluator=evaluator_clf,\n",
        "    numFolds=3\n",
        ")\n",
        "\n",
        "cv_clf_model = cv_clf.fit(train)\n",
        "cv_pred = cv_clf_model.transform(test)\n",
        "\n",
        "auc_cv = evaluator_clf.evaluate(cv_pred)\n",
        "print(\"AUC Logistic Regression (Cross-Validated) =\", auc_cv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCkqnL0uCoQ1",
        "outputId": "b92a8d0d-6c30-4fa4-a776-59705afa2fd3"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC Logistic Regression (Cross-Validated) = 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear regression\n",
        "param_grid_reg = (ParamGridBuilder()\n",
        "                  .addGrid(lr_reg.regParam, [0.01, 0.1, 1.0])\n",
        "                  .addGrid(lr_reg.elasticNetParam, [0.0, 0.5, 1.0])\n",
        "                  .build())\n",
        "\n",
        "cv_reg = CrossValidator(\n",
        "    estimator=pipeline_reg,\n",
        "    estimatorParamMaps=param_grid_reg,\n",
        "    evaluator=evaluator_reg,\n",
        "    numFolds=3\n",
        ")\n",
        "\n",
        "cv_reg_model = cv_reg.fit(train)\n",
        "cv_reg_pred = cv_reg_model.transform(test)\n",
        "\n",
        "rmse_cv = evaluator_reg.evaluate(cv_reg_pred)\n",
        "print(\"RMSE Linear Regression (Cross-Validated) =\", rmse_cv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pw_Uin7FH_C2",
        "outputId": "64108729-942a-4757-8294-ddb420497e9a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE Linear Regression (Cross-Validated) = 0.008741489800610154\n"
          ]
        }
      ]
    }
  ]
}